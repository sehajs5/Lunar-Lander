{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtMa3GWB_-za"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd\n",
        "from torch.autograd import Variable\n",
        "from collections import deque, namedtuple\n",
        "import gymnasium as gym\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "import imageio\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "class DQNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, seed=42):\n",
        "        super(DQNetwork, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.layer1 = nn.Linear(input_dim, 64)\n",
        "        self.layer2 = nn.Linear(64, 64)\n",
        "        self.layer3 = nn.Linear(64, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = F.relu(self.layer2(x))\n",
        "        return self.layer3(x)\n",
        "\n",
        "class ExperienceBuffer:\n",
        "    def __init__(self, max_size):\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.max_size = max_size\n",
        "        self.buffer = []\n",
        "\n",
        "    def add(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "        if len(self.buffer) > self.max_size:\n",
        "            self.buffer.pop(0)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        experiences = random.sample(self.buffer, k=batch_size)\n",
        "        states = torch.from_numpy(np.vstack([e[0] for e in experiences if e is not None])).float().to(self.device)\n",
        "        actions = torch.from_numpy(np.vstack([e[1] for e in experiences if e is not None])).long().to(self.device)\n",
        "        rewards = torch.from_numpy(np.vstack([e[2] for e in experiences if e is not None])).float().to(self.device)\n",
        "        next_states = torch.from_numpy(np.vstack([e[3] for e in experiences if e is not None])).float().to(self.device)\n",
        "        dones = torch.from_numpy(np.vstack([e[4] for e in experiences if e is not None]).astype(np.uint8)).float().to(self.device)\n",
        "        return states, actions, rewards, next_states, dones\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size, learning_rate=5e-4, buffer_size=int(1e5), batch_size=100, gamma=0.99, tau=1e-3):\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.qnetwork_local = DQNetwork(state_size, action_size).to(self.device)\n",
        "        self.qnetwork_target = DQNetwork(state_size, action_size).to(self.device)\n",
        "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=learning_rate)\n",
        "        self.memory = ExperienceBuffer(buffer_size)\n",
        "        self.t_step = 0\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        self.memory.add((state, action, reward, next_state, done))\n",
        "        self.t_step = (self.t_step + 1) % 4\n",
        "        if self.t_step == 0 and len(self.memory.buffer) > self.batch_size:\n",
        "            experiences = self.memory.sample(self.batch_size)\n",
        "            self.learn(experiences)\n",
        "\n",
        "    def act(self, state, epsilon=0.):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
        "        self.qnetwork_local.eval()\n",
        "        with torch.no_grad():\n",
        "            action_values = self.qnetwork_local(state)\n",
        "        self.qnetwork_local.train()\n",
        "        if random.random() > epsilon:\n",
        "            return np.argmax(action_values.cpu().data.numpy())\n",
        "        else:\n",
        "            return random.choice(np.arange(self.action_size))\n",
        "\n",
        "    def learn(self, experiences):\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "        q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "        q_targets = rewards + self.gamma * q_targets_next * (1 - dones)\n",
        "        q_expected = self.qnetwork_local(states).gather(1, actions)\n",
        "        loss = F.mse_loss(q_expected, q_targets)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.soft_update(self.qnetwork_local, self.qnetwork_target, self.tau)\n",
        "\n",
        "    def soft_update(self, local_model, target_model, tau):\n",
        "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n",
        "\n",
        "def train_dqn(env_name, n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
        "    env = gym.make(env_name)\n",
        "    state_size = env.observation_space.shape[0]\n",
        "    action_size = env.action_space.n\n",
        "    agent = DQNAgent(state_size, action_size)\n",
        "\n",
        "    scores = []\n",
        "    scores_window = deque(maxlen=100)\n",
        "    eps = eps_start\n",
        "\n",
        "    for i_episode in range(1, n_episodes+1):\n",
        "        state, _ = env.reset()\n",
        "        score = 0\n",
        "        for t in range(max_t):\n",
        "            action = agent.act(state, eps)\n",
        "            next_state, reward, done, _, _ = env.step(action)\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            score += reward\n",
        "            if done:\n",
        "                break\n",
        "        scores_window.append(score)\n",
        "        scores.append(score)\n",
        "        eps = max(eps_end, eps_decay*eps)\n",
        "        print(f'\\rEpisode {i_episode}\\tAverage Score: {np.mean(scores_window):.2f}', end=\"\")\n",
        "        if i_episode % 100 == 0:\n",
        "            print(f'\\rEpisode {i_episode}\\tAverage Score: {np.mean(scores_window):.2f}')\n",
        "        if np.mean(scores_window) >= 200.0:\n",
        "            print(f'\\nEnvironment solved in {i_episode-100:d} episodes!\\tAverage Score: {np.mean(scores_window):.2f}')\n",
        "            torch.save(agent.qnetwork_local.state_dict(), 'dqn_checkpoint.pth')\n",
        "            break\n",
        "    return agent, scores\n",
        "\n",
        "def visualize_agent(agent, env_name):\n",
        "    env = gym.make(env_name, render_mode='rgb_array')\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    frames = []\n",
        "    while not done:\n",
        "        frame = env.render()\n",
        "        frames.append(frame)\n",
        "        action = agent.act(state)\n",
        "        state, reward, done, _, _ = env.step(action)\n",
        "    env.close()\n",
        "    imageio.mimsave('agent_video.mp4', frames, fps=30)\n",
        "\n",
        "def display_video():\n",
        "    mp4list = glob.glob('*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        display(HTML(data='''<video alt=\"agent video\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print(\"No video file found\")\n",
        "\n",
        "# Train the agent\n",
        "trained_agent, scores = train_dqn('LunarLander-v2')\n",
        "\n",
        "# Visualize the trained agent\n",
        "visualize_agent(trained_agent, 'LunarLander-v2')\n",
        "\n",
        "# Display the video\n",
        "display_video()"
      ]
    }
  ]
}